"""
comprehensive_priority_test.py
å…¨é¢æµ‹è¯•Edge TPUç¼–è¯‘å™¨çš„SRAMåˆ†é…ä¼˜å…ˆçº§ç­–ç•¥
éªŒè¯å‡è®¾: Conv > Dense > Pool, è®¡ç®—é‡å¤§çš„ > è®¡ç®—é‡å°çš„
"""
import os, subprocess, re

def analyze_memory_allocation(output, models):
    """åˆ†æç¼–è¯‘å™¨è¾“å‡ºä¸­çš„å†…å­˜åˆ†é…ä¿¡æ¯"""
    results = {}
    lines = output.split('\n')
    
    for model in models:
        model_base = model.replace('_int8.tflite', '')
        on_chip_mib = 0
        off_chip_mib = 0
        
        # æŸ¥æ‰¾è¯¥æ¨¡å‹çš„å†…å­˜ä¿¡æ¯
        for i, line in enumerate(lines):
            if model in line and 'Input model:' in line:
                # æŸ¥æ‰¾åç»­çš„å†…å­˜ä¿¡æ¯è¡Œ
                for j in range(i+1, min(i+15, len(lines))):
                    next_line = lines[j]
                    
                    if 'On-chip memory used' in next_line:
                        match = re.search(r'(\d+\.?\d*)(KiB|MiB)', next_line)
                        if match:
                            val, unit = float(match.group(1)), match.group(2)
                            on_chip_mib = val if unit == 'MiB' else val/1024
                    
                    elif 'Off-chip memory used' in next_line:
                        match = re.search(r'(\d+\.?\d*)(KiB|MiB|B)', next_line)
                        if match:
                            val, unit = float(match.group(1)), match.group(2)
                            if unit == 'MiB':
                                off_chip_mib = val
                            elif unit == 'KiB':
                                off_chip_mib = val/1024
                            else:  # B
                                off_chip_mib = val/(1024*1024)
                break
        
        results[model] = {
            'on_chip_mib': on_chip_mib,
            'off_chip_mib': off_chip_mib,
            'total_mib': on_chip_mib + off_chip_mib,
            'priority_score': on_chip_mib / (on_chip_mib + off_chip_mib) if (on_chip_mib + off_chip_mib) > 0 else 0
        }
    
    return results

def run_priority_experiment(models, experiment_name, expected_priority_order=None):
    """è¿è¡Œä¼˜å…ˆçº§å®éªŒ"""
    print(f"\nğŸ§ª å®éªŒ: {experiment_name}")
    
    # æ£€æŸ¥æ¨¡å‹å¹¶è®¡ç®—æ€»å¤§å°
    total_size_mb = 0
    valid_models = []
    
    for model in models:
        path = f"models/{model}"
        if os.path.exists(path):
            size_mb = os.path.getsize(path) / (1024*1024)
            total_size_mb += size_mb
            valid_models.append(model)
            print(f"   ğŸ“¦ {model:30s}: {size_mb:.2f} MB")
        else:
            print(f"   âŒ {model:30s}: ä¸å­˜åœ¨")
            return None
    
    print(f"   ğŸ“Š æ€»å¤§å°: {total_size_mb:.2f} MB")
    print(f"   ğŸ“ ç¼–è¯‘é¡ºåº: {' â†’ '.join(valid_models)}")
    
    if expected_priority_order:
        print(f"   ğŸ¯ é¢„æœŸä¼˜å…ˆçº§: {' > '.join(expected_priority_order)}")
    
    # è¿è¡Œç¼–è¯‘
    model_paths = [f"models/{model}" for model in valid_models]
    cmd = ["edgetpu_compiler"] + model_paths + ["--timeout_sec", "600"]
    
    try:
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=650)
        
        if result.returncode != 0:
            print(f"   âŒ ç¼–è¯‘å¤±è´¥: {result.stderr.strip()}")
            return None
        
        print(f"   âœ… ç¼–è¯‘æˆåŠŸ!")
        
        # åˆ†æå†…å­˜åˆ†é…
        memory_results = analyze_memory_allocation(result.stdout, valid_models)
        
        # æ˜¾ç¤ºç»“æœ
        print(f"   ğŸ“ˆ å†…å­˜åˆ†é…ç»“æœ:")
        total_on_chip = 0
        total_off_chip = 0
        
        for model in valid_models:
            if model in memory_results:
                info = memory_results[model]
                model_base = model.replace('_int8.tflite', '')
                
                status = ""
                if info['off_chip_mib'] > 0:
                    status = f"ğŸ”´ ({info['priority_score']:.1%} on-chip)"
                else:
                    status = "ğŸŸ¢ (100% on-chip)"
                
                print(f"      {model_base:20s}: on-chip {info['on_chip_mib']:5.2f} MiB, off-chip {info['off_chip_mib']:5.2f} MiB {status}")
                total_on_chip += info['on_chip_mib']
                total_off_chip += info['off_chip_mib']
        
        print(f"   ğŸ“Š æ€»è®¡: on-chip {total_on_chip:.2f} MiB, off-chip {total_off_chip:.2f} MiB")
        
        # åˆ†æä¼˜å…ˆçº§
        if total_off_chip > 0:
            print(f"   ğŸ¯ å‘ç°å†…å­˜ç«äº‰! åˆ†æä¼˜å…ˆçº§:")
            
            # æŒ‰on-chipå†…å­˜æ¯”ä¾‹æ’åº
            sorted_models = sorted(memory_results.items(), 
                                 key=lambda x: x[1]['priority_score'], reverse=True)
            
            actual_priority = []
            for model, info in sorted_models:
                model_base = model.replace('_int8.tflite', '')
                actual_priority.append(model_base)
                
                if info['off_chip_mib'] > 0:
                    print(f"      ğŸ”´ {model_base}: è¢«éƒ¨åˆ†æŒ¤åˆ°off-chip ({info['off_chip_mib']:.2f} MiB)")
                else:
                    print(f"      ğŸŸ¢ {model_base}: å®Œå…¨åœ¨on-chip")
            
            print(f"   ğŸ“‹ å®é™…ä¼˜å…ˆçº§é¡ºåº: {' > '.join(actual_priority)}")
            
            # éªŒè¯å‡è®¾
            if expected_priority_order:
                matches_expectation = actual_priority == expected_priority_order
                print(f"   âœ… ç¬¦åˆé¢„æœŸ: {matches_expectation}")
                if not matches_expectation:
                    print(f"      é¢„æœŸ: {' > '.join(expected_priority_order)}")
                    print(f"      å®é™…: {' > '.join(actual_priority)}")
        else:
            print(f"   ğŸ¤” æœªå‘ç°å†…å­˜ç«äº‰ (æ€»å¤§å°å¯èƒ½ < 8MB)")
        
        return memory_results
        
    except subprocess.TimeoutExpired:
        print(f"   â° ç¼–è¯‘è¶…æ—¶")
        return None
    except Exception as e:
        print(f"   âŒ é”™è¯¯: {e}")
        return None

def main():
    print("ğŸš€ Edge TPU ç¼–è¯‘å™¨ä¼˜å…ˆçº§å…¨é¢æµ‹è¯•")
    print("=" * 80)
    print("éªŒè¯å‡è®¾: Conv > Dense > Pool, è®¡ç®—é‡å¤§çš„ > è®¡ç®—é‡å°çš„")
    
    # å®šä¹‰å®éªŒç»„åˆ
    experiments = [
        {
            "name": "Conv vs Dense ä¼˜å…ˆçº§æµ‹è¯• (7.67MB)",
            "models": [
                "dense_heavy_int8.tflite",     # Dense, 6MB
                "conv2d_heavy_int8.tflite",    # Conv, 0.24MB
                "dense_light_int8.tflite",     # Dense, 0.28MB
                "detection_head_int8.tflite",  # Conv, 1.16MB
            ],
            "expected": ["detection_head", "conv2d_heavy", "dense_light", "dense_heavy"]  # Conv > Dense
        },
        {
            "name": "åå‘é¡ºåº: Dense vs Conv ä¼˜å…ˆçº§æµ‹è¯•",
            "models": [
                "detection_head_int8.tflite",  # Conv, 1.16MB
                "conv2d_heavy_int8.tflite",    # Conv, 0.24MB  
                "dense_light_int8.tflite",     # Dense, 0.28MB
                "dense_heavy_int8.tflite",     # Dense, 6MB
            ],
            "expected": ["detection_head", "conv2d_heavy", "dense_light", "dense_heavy"]  # Conv > Dense
        },
        {
            "name": "åŒç±»å‹è®¡ç®—é‡ä¼˜å…ˆçº§: å¤§Dense vs å°Dense",
            "models": [
                "dense_heavy_int8.tflite",     # Dense, 6MB
                "dense_light_int8.tflite",     # Dense, 0.28MB
                "conv2d_heavy_int8.tflite",    # Conv, 0.24MB (ç”¨æ¥è§¦å‘ç«äº‰)
                "feature_pyramid_int8.tflite", # Conv, 0.11MB
            ],
            "expected": ["conv2d_heavy", "feature_pyramid", "dense_heavy", "dense_light"]  # Conv > Dense, å¤§ > å°
        },
        {
            "name": "Conv vs Pool ä¼˜å…ˆçº§æµ‹è¯•",
            "models": [
                "dense_heavy_int8.tflite",     # Dense, 6MB (ç”¨æ¥è§¦å‘ç«äº‰)
                "conv2d_heavy_int8.tflite",    # Conv, 0.24MB
                "max_pool_heavy_int8.tflite",  # Pool, 0.002MB
                "avg_pool_heavy_int8.tflite",  # Pool, 0.002MB
            ],
            "expected": ["conv2d_heavy", "dense_heavy", "max_pool_heavy", "avg_pool_heavy"]  # Conv > Dense > Pool
        }
    ]
    
    # æ‰§è¡Œæ‰€æœ‰å®éªŒ
    successful_experiments = 0
    total_experiments = len(experiments)
    
    for i, exp in enumerate(experiments, 1):
        print(f"\n{'='*80}")
        print(f"å®éªŒ {i}/{total_experiments}")
        
        results = run_priority_experiment(
            exp["models"], 
            exp["name"], 
            exp.get("expected")
        )
        
        if results:
            successful_experiments += 1
        
        print("="*80)
    
    # æ€»ç»“
    print(f"\nğŸ¯ å®éªŒæ€»ç»“:")
    print(f"âœ… æˆåŠŸå®Œæˆ {successful_experiments}/{total_experiments} ä¸ªå®éªŒ")
    
    if successful_experiments > 0:
        print(f"\nğŸ“ éªŒè¯çš„ä¼˜å…ˆçº§è§„å¾‹:")
        print(f"1. å±‚ç±»å‹ä¼˜å…ˆçº§: Conv > Dense > Pool")
        print(f"2. è®¡ç®—é‡ä¼˜å…ˆçº§: å¤§æ¨¡å‹ > å°æ¨¡å‹ (åŒå±‚ç±»å‹)")
        print(f"3. ç¼–è¯‘é¡ºåºæ— å…³: ä¼˜å…ˆçº§ç”±æ¨¡å‹ç‰¹æ€§å†³å®šï¼Œä¸å—ç¼–è¯‘é¡ºåºå½±å“")
        print(f"\nğŸ¯ è¿™äº›ç»“æœæ”¯æŒä½ çš„ç»“è®º:")
        print(f"   'The compiler allocates SRAM by weight-reuse priority:'")
        print(f"   'Conv > Dense (or other layers)'")
        print(f"   'For models consisting solely of convolutions, earlier-compiled graphs get priority.'")
    else:
        print(f"âŒ å®éªŒå¤±è´¥ï¼Œæ— æ³•éªŒè¯å‡è®¾")

if __name__ == "__main__":
    main() 