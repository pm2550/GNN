{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已检测到GPU: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "已启用GPU内存动态增长\n",
      "加载知识图谱数据...\n",
      "从 ../wiki\\train.txt 加载了 20614279 个三元组\n",
      "从 ../wiki\\test.txt 加载了 5133 个三元组\n",
      "从 ../wiki\\valid.txt 加载了 5163 个三元组\n",
      "\n",
      "从训练集、验证集和测试集中提取实体和关系ID...\n",
      "从所有数据集中提取了 4594485 个实体和 822 个关系\n",
      "\n",
      "加载实体和关系（带别名）...\n",
      "从 ../wiki\\entity.txt 加载了 4813491 个ID，共 28835154 个别名\n",
      "发现 4844 个在三元组中出现但文件中不存在的ID\n",
      "最终包含了 4818335 个ID\n",
      "从 ../wiki\\relation.txt 加载了 825 个ID，共 4232 个别名\n",
      "发现 3 个在三元组中出现但文件中不存在的ID\n",
      "最终包含了 828 个ID\n",
      "\n",
      "准备编码器...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LabelEncoder</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.preprocessing.LabelEncoder.html\">?<span>Documentation for LabelEncoder</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>LabelEncoder()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import spektral\n",
    "from spektral.layers import GCNConv, GATConv\n",
    "from spektral.models.gcn import GCN\n",
    "from spektral.data import Graph\n",
    "from spektral.data.loaders import SingleLoader\n",
    "\n",
    "# 检查GPU并启用内存增长\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    print(f\"已检测到GPU: {physical_devices}\")\n",
    "    for device in physical_devices:\n",
    "        tf.config.experimental.set_memory_growth(device, True)\n",
    "    print(\"已启用GPU内存动态增长\")\n",
    "else:\n",
    "    print(\"未检测到GPU，将使用CPU\")\n",
    "\n",
    "# 设置路径\n",
    "WIKI_DIR = \"../wiki\"\n",
    "MODEL_DIR = \"../models\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# 1. 加载三元组数据\n",
    "print(\"加载知识图谱数据...\")\n",
    "\n",
    "def load_triples(file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, sep='\\t', header=None, \n",
    "                         names=['head', 'relation', 'tail'])\n",
    "        print(f\"从 {file_path} 加载了 {len(df)} 个三元组\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"无法加载 {file_path}: {e}\")\n",
    "        return pd.DataFrame(columns=['head', 'relation', 'tail'])\n",
    "\n",
    "train_df = load_triples(os.path.join(WIKI_DIR, 'train.txt'))\n",
    "test_df = load_triples(os.path.join(WIKI_DIR, 'test.txt'))\n",
    "valid_df = load_triples(os.path.join(WIKI_DIR, 'valid.txt'))\n",
    "\n",
    "# 2. 从训练集、验证集和测试集中提取所有实体和关系\n",
    "print(\"\\n从训练集、验证集和测试集中提取实体和关系ID...\")\n",
    "all_dataset_entities = pd.concat([\n",
    "    train_df['head'], train_df['tail'],\n",
    "    valid_df['head'] if not valid_df.empty else pd.Series(),\n",
    "    valid_df['tail'] if not valid_df.empty else pd.Series(), \n",
    "    test_df['head'] if not test_df.empty else pd.Series(),\n",
    "    test_df['tail'] if not test_df.empty else pd.Series()\n",
    "]).unique().tolist()\n",
    "\n",
    "all_dataset_relations = pd.concat([\n",
    "    train_df['relation'],\n",
    "    valid_df['relation'] if not valid_df.empty else pd.Series(),\n",
    "    test_df['relation'] if not test_df.empty else pd.Series()\n",
    "]).unique().tolist()\n",
    "\n",
    "print(f\"从所有数据集中提取了 {len(all_dataset_entities)} 个实体和 {len(all_dataset_relations)} 个关系\")\n",
    "\n",
    "# 3. 加载实体和关系数据，并确保三元组中的所有ID都包含\n",
    "print(\"\\n加载实体和关系（带别名）...\")\n",
    "\n",
    "def load_entities_with_aliases(file_path, all_ids_from_triples):\n",
    "    \"\"\"加载实体或关系文件，保留ID和所有别名，并确保包含三元组中的所有ID\"\"\"\n",
    "    id_to_aliases = {}\n",
    "    aliases_to_id = {}\n",
    "    all_ids = set()\n",
    "    \n",
    "    # 首先尝试从文件加载\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split('\\t')\n",
    "                if len(parts) < 1:\n",
    "                    continue\n",
    "                \n",
    "                entity_id = parts[0]\n",
    "                aliases = parts[1:] if len(parts) > 1 else [entity_id]\n",
    "                \n",
    "                # 添加ID作为默认别名\n",
    "                if entity_id not in aliases:\n",
    "                    aliases.append(entity_id)\n",
    "                \n",
    "                id_to_aliases[entity_id] = aliases\n",
    "                all_ids.add(entity_id)\n",
    "                \n",
    "                # 为每个别名创建反向映射\n",
    "                for alias in aliases:\n",
    "                    aliases_to_id[alias] = entity_id\n",
    "        \n",
    "        print(f\"从 {file_path} 加载了 {len(id_to_aliases)} 个ID，共 {len(aliases_to_id)} 个别名\")\n",
    "    except Exception as e:\n",
    "        print(f\"无法从文件加载别名: {e}\")\n",
    "    \n",
    "    # 确保包含三元组中的所有ID\n",
    "    missing_ids = set(all_ids_from_triples) - all_ids\n",
    "    if missing_ids:\n",
    "        print(f\"发现 {len(missing_ids)} 个在三元组中出现但文件中不存在的ID\")\n",
    "        for entity_id in missing_ids:\n",
    "            id_to_aliases[entity_id] = [entity_id]\n",
    "            aliases_to_id[entity_id] = entity_id\n",
    "            all_ids.add(entity_id)\n",
    "    \n",
    "    print(f\"最终包含了 {len(id_to_aliases)} 个ID\")\n",
    "    return id_to_aliases, aliases_to_id, list(id_to_aliases.keys())\n",
    "\n",
    "# 加载实体和关系文件\n",
    "entity_file = os.path.join(WIKI_DIR, 'entity.txt')\n",
    "relation_file = os.path.join(WIKI_DIR, 'relation.txt')\n",
    "\n",
    "entity_id_to_aliases, entity_alias_to_id, entity_ids = load_entities_with_aliases(\n",
    "    entity_file, all_dataset_entities\n",
    ")\n",
    "\n",
    "relation_id_to_aliases, relation_alias_to_id, relation_ids = load_entities_with_aliases(\n",
    "    relation_file, all_dataset_relations\n",
    ")\n",
    "\n",
    "# 4. 编码实体和关系\n",
    "print(\"\\n准备编码器...\")\n",
    "entity_encoder = LabelEncoder()\n",
    "relation_encoder = LabelEncoder()\n",
    "\n",
    "entity_encoder.fit(entity_ids)\n",
    "relation_encoder.fit(relation_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "建立图结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "构建图结构...\n",
      "实体数量: 4818335, 关系数量: 828\n",
      "初始化增强的节点特征...\n"
     ]
    }
   ],
   "source": [
    "# 5. 构建图结构 (适用于Spektral)\n",
    "print(\"\\n构建图结构...\")\n",
    "num_entities = len(entity_encoder.classes_)\n",
    "num_relations = len(relation_encoder.classes_)\n",
    "\n",
    "print(f\"实体数量: {num_entities}, 关系数量: {num_relations}\")\n",
    "\n",
    "# 准备节点特征 (结合随机和位置编码)\n",
    "print(\"初始化增强的节点特征...\")\n",
    "feature_dim = 32  # 增加到32维，提供更丰富的表示能力\n",
    "\n",
    "# 使用更复杂的节点特征初始化 - 结合位置编码\n",
    "node_features = np.zeros((num_entities, feature_dim))\n",
    "for i in range(num_entities):\n",
    "    # 半随机半结构化特征\n",
    "    for j in range(feature_dim):\n",
    "        if j % 2 == 0:\n",
    "            # 位置编码：基于实体ID的正弦编码\n",
    "            node_features[i, j] = np.sin(i / (10000 ** (j / feature_dim)))\n",
    "        else:\n",
    "            # 位置编码：基于实体ID的余弦编码\n",
    "            node_features[i, j] = np.cos(i / (10000 ** ((j-1) / feature_dim)))\n",
    "    \n",
    "    # 添加少量噪声以增加多样性\n",
    "    node_features[i] += np.random.normal(0, 0.1, feature_dim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "创建邻接矩阵...\n",
      "优先包含了 15417 个验证/测试实体, 总采样 500000 个实体\n",
      "过滤后的边数量: 1324730 (原始: 20614279)\n",
      "采样后图的节点数: 500000, 边数: 1324730\n"
     ]
    }
   ],
   "source": [
    "# 创建邻接矩阵\n",
    "print(\"创建邻接矩阵...\")\n",
    "edges_src = entity_encoder.transform(train_df['head'])\n",
    "edges_dst = entity_encoder.transform(train_df['tail'])\n",
    "edge_types = relation_encoder.transform(train_df['relation'])\n",
    "\n",
    "# 定义最大节点数\n",
    "max_nodes = 500000  # 最大采样50万个节点\n",
    "\n",
    "# 替换原随机采样代码，使用顺序采样或优先采样策略\n",
    "if num_entities <= max_nodes:\n",
    "    # 如果实体数不超过最大节点数，直接使用所有实体\n",
    "    sampled_indices = np.arange(num_entities)\n",
    "    print(f\"使用全部 {num_entities} 个实体，无需采样\")\n",
    "else:\n",
    "    # 优先包含验证集和测试集的实体\n",
    "    priority_entities = set()\n",
    "    \n",
    "    # 收集验证集和测试集的实体\n",
    "    for df in [valid_df, test_df]:\n",
    "        if not df.empty:\n",
    "            for _, row in df.iterrows():\n",
    "                if row['head'] in entity_alias_to_id:\n",
    "                    priority_entities.add(entity_alias_to_id[row['head']])\n",
    "                if row['tail'] in entity_alias_to_id:\n",
    "                    priority_entities.add(entity_alias_to_id[row['tail']])\n",
    "    \n",
    "    # 转换为索引\n",
    "    priority_indices = []\n",
    "    for entity_id in priority_entities:\n",
    "        if entity_id in entity_encoder.classes_:\n",
    "            idx = np.where(entity_encoder.classes_ == entity_id)[0][0]\n",
    "            priority_indices.append(idx)\n",
    "    \n",
    "    priority_indices = np.array(priority_indices)\n",
    "    \n",
    "    # 随机采样剩余的实体\n",
    "    remaining_count = max_nodes - len(priority_indices)\n",
    "    if remaining_count > 0:\n",
    "        # 获取未优先的索引\n",
    "        all_indices = np.arange(num_entities)\n",
    "        mask = np.ones(num_entities, dtype=bool)\n",
    "        mask[priority_indices] = False\n",
    "        remaining_indices = all_indices[mask]\n",
    "        \n",
    "        # 随机采样剩余索引\n",
    "        sampled_remaining = np.random.choice(\n",
    "            remaining_indices, \n",
    "            min(remaining_count, len(remaining_indices)), \n",
    "            replace=False\n",
    "        )\n",
    "        \n",
    "        # 合并优先索引和随机采样的索引\n",
    "        sampled_indices = np.concatenate([priority_indices, sampled_remaining])\n",
    "    else:\n",
    "        # 如果优先实体已经超过最大节点数，只使用优先实体\n",
    "        sampled_indices = np.random.choice(priority_indices, max_nodes, replace=False)\n",
    "    \n",
    "    print(f\"优先包含了 {len(priority_indices)} 个验证/测试实体, 总采样 {len(sampled_indices)} 个实体\")\n",
    "\n",
    "# 提取采样节点的特征\n",
    "node_features = node_features[sampled_indices]\n",
    "\n",
    "# 创建索引映射字典\n",
    "idx_map = {old_idx: new_idx for new_idx, old_idx in enumerate(sampled_indices)}\n",
    "\n",
    "# 过滤只包含采样节点的边\n",
    "mask_src = np.isin(edges_src, sampled_indices)\n",
    "mask_dst = np.isin(edges_dst, sampled_indices)\n",
    "mask = mask_src & mask_dst\n",
    "\n",
    "edges_src_filtered = edges_src[mask]\n",
    "edges_dst_filtered = edges_dst[mask]\n",
    "edge_types_filtered = edge_types[mask]\n",
    "\n",
    "# 重映射节点索引\n",
    "edges_src_mapped = np.array([idx_map[idx] for idx in edges_src_filtered])\n",
    "edges_dst_mapped = np.array([idx_map[idx] for idx in edges_dst_filtered])\n",
    "\n",
    "print(f\"过滤后的边数量: {len(edges_src_mapped)} (原始: {len(edges_src)})\")\n",
    "\n",
    "# 使用scipy创建稀疏邻接矩阵\n",
    "from scipy.sparse import csr_matrix\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# 创建稀疏邻接矩阵\n",
    "data = np.ones(len(edges_src_mapped))\n",
    "adj = sp.coo_matrix((data, (edges_src_mapped, edges_dst_mapped)), \n",
    "                   shape=(len(sampled_indices), len(sampled_indices)))\n",
    "adj = adj.tocsr()  # 转换为CSR格式提高性能\n",
    "\n",
    "# 更新实体数\n",
    "num_entities = len(sampled_indices)\n",
    "print(f\"采样后图的节点数: {num_entities}, 边数: {len(edges_src_mapped)}\")\n",
    "\n",
    "# 更新训练用边的信息\n",
    "edges_src = edges_src_mapped\n",
    "edges_dst = edges_dst_mapped\n",
    "edge_types = edge_types_filtered\n",
    "\n",
    "# 将SciPy稀疏矩阵转换为TensorFlow稀疏张量\n",
    "def scipy_sparse_to_tf_sparse(sparse_mx):\n",
    "    \"\"\"将scipy稀疏矩阵转换为tensorflow稀疏张量，确保索引正确排序\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    \n",
    "    # 获取行列索引和值\n",
    "    indices = np.column_stack((sparse_mx.row, sparse_mx.col))\n",
    "    values = sparse_mx.data\n",
    "    shape = sparse_mx.shape\n",
    "    \n",
    "    # 对索引进行排序\n",
    "    sorted_idx = np.lexsort((indices[:, 1], indices[:, 0]))\n",
    "    indices = indices[sorted_idx]\n",
    "    values = values[sorted_idx]\n",
    "    \n",
    "    # 创建TensorFlow稀疏张量\n",
    "    return tf.sparse.SparseTensor(indices=indices, values=values, dense_shape=shape)\n",
    "\n",
    "# 转换邻接矩阵为TensorFlow稀疏张量\n",
    "adj_tensor = scipy_sparse_to_tf_sparse(adj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "创建（无 KGE）多关系 GNN 模型...\n",
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " node_features_input (InputLaye  [(None, 32)]        0           []                               \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 32)           1056        ['node_features_input[0][0]']    \n",
      "                                                                                                  \n",
      " adj_input_r0 (InputLayer)      [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " my_relational_conv_6 (MyRelati  (None, 32)          1024        ['dense_6[0][0]',                \n",
      " onalConv)                                                        'adj_input_r0[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 32)           0           ['my_relational_conv_6[0][0]']   \n",
      "                                                                                                  \n",
      " tf.nn.relu_6 (TFOpLambda)      (None, 32)           0           ['dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " my_relational_conv_7 (MyRelati  (None, 32)          1024        ['tf.nn.relu_6[0][0]',           \n",
      " onalConv)                                                        'adj_input_r0[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 32)           0           ['my_relational_conv_7[0][0]']   \n",
      "                                                                                                  \n",
      " tf.nn.relu_7 (TFOpLambda)      (None, 32)           0           ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 32)           1056        ['tf.nn.relu_7[0][0]']           \n",
      "                                                                                                  \n",
      " head_indices (InputLayer)      [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " tail_indices (InputLayer)      [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather_6 (TFOpLam  (None, 32)          0           ['dense_7[0][0]',                \n",
      " bda)                                                             'head_indices[0][0]']           \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather_7 (TFOpLam  (None, 32)          0           ['dense_7[0][0]',                \n",
      " bda)                                                             'tail_indices[0][0]']           \n",
      "                                                                                                  \n",
      " tf.math.multiply_3 (TFOpLambda  (None, 32)          0           ['tf.compat.v1.gather_6[0][0]',  \n",
      " )                                                                'tf.compat.v1.gather_7[0][0]']  \n",
      "                                                                                                  \n",
      " tf.math.reduce_sum_3 (TFOpLamb  (None,)             0           ['tf.math.multiply_3[0][0]']     \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_3 (TFOpLambda)  (None,)             0           ['tf.math.reduce_sum_3[0][0]']   \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,160\n",
      "Trainable params: 4,160\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " node_features_input (InputLaye  [(None, 32)]        0           []                               \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 32)           1056        ['node_features_input[0][0]']    \n",
      "                                                                                                  \n",
      " adj_input_r0 (InputLayer)      [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " my_relational_conv_6 (MyRelati  (None, 32)          1024        ['dense_6[0][0]',                \n",
      " onalConv)                                                        'adj_input_r0[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 32)           0           ['my_relational_conv_6[0][0]']   \n",
      "                                                                                                  \n",
      " tf.nn.relu_6 (TFOpLambda)      (None, 32)           0           ['dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " my_relational_conv_7 (MyRelati  (None, 32)          1024        ['tf.nn.relu_6[0][0]',           \n",
      " onalConv)                                                        'adj_input_r0[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 32)           0           ['my_relational_conv_7[0][0]']   \n",
      "                                                                                                  \n",
      " tf.nn.relu_7 (TFOpLambda)      (None, 32)           0           ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 32)           1056        ['tf.nn.relu_7[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,160\n",
      "Trainable params: 4,160\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "尝试一次前向传播来检查形状...\n",
      "前向传播成功! 预测形状: (2,)\n",
      "嵌入模型前向传播成功! 节点嵌入形状: (500000, 32)\n"
     ]
    }
   ],
   "source": [
    "# 6. 创建标准GNN模型 (去掉 KGE, 不再使用 DistMult)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import Dense, Dropout, Layer\n",
    "\n",
    "##########################\n",
    "# 自定义多关系卷积层 (MyRelationalConv)\n",
    "##########################\n",
    "class MyRelationalConv(Layer):\n",
    "    def __init__(self, hidden_dim, num_relations, **kwargs):\n",
    "        \"\"\"\n",
    "        多关系卷积：对每种关系有一套权重 W_r，\n",
    "        然后对每种关系的邻接矩阵分别做 A_r * (X * W_r)，再把结果相加。\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_relations = num_relations\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # 为每种关系单独的权重矩阵 W_r\n",
    "        self.relation_weights = []\n",
    "        for r in range(num_relations):\n",
    "            w = self.add_weight(\n",
    "                shape=(hidden_dim, hidden_dim),\n",
    "                initializer='glorot_uniform',\n",
    "                trainable=True,\n",
    "                name=f\"W_rel_{r}\"\n",
    "            )\n",
    "            self.relation_weights.append(w)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        inputs = [x] + [adj_r0, adj_r1, ..., adj_r{num_relations-1}]\n",
    "        x: (N, hidden_dim)\n",
    "        adj_rk: (N, N), sparse or dense\n",
    "        \"\"\"\n",
    "        x = inputs[0]\n",
    "        adjs = inputs[1:]  # list of adjacency for each relation\n",
    "        outputs = 0\n",
    "        # 分别对每种关系做卷积\n",
    "        for r_idx, adj in enumerate(adjs):\n",
    "            w_r = self.relation_weights[r_idx]  # (hidden_dim, hidden_dim)\n",
    "            xw = tf.matmul(x, w_r)              # (N, hidden_dim)\n",
    "\n",
    "            # A_r * (X * W_r)\n",
    "            if isinstance(adj, tf.SparseTensor):\n",
    "                ax = tf.sparse.sparse_dense_matmul(adj, xw)\n",
    "            else:\n",
    "                ax = tf.matmul(adj, xw)\n",
    "            outputs += ax\n",
    "        return outputs\n",
    "\n",
    "\n",
    "##########################\n",
    "# 创建一个去掉 KGE 的多关系链路预测模型\n",
    "##########################\n",
    "def create_multi_rel_gnn_no_kge(\n",
    "    num_entities,\n",
    "    num_relations,\n",
    "    node_features_dim,\n",
    "    hidden_dim=32\n",
    "):\n",
    "    \"\"\"\n",
    "    不再使用 DistMult 等 KGE 方法，\n",
    "    仅在最后对 (head, tail) 的节点向量做一个简单打分（点积 + sigmoid）。\n",
    "    假设你已经准备好多关系邻接矩阵列表，每个关系一份。\n",
    "    \"\"\"\n",
    "\n",
    "    # --------- 定义输入 ---------\n",
    "    # 1) 节点特征输入 (N, node_features_dim)\n",
    "    node_features_input = Input(shape=(node_features_dim,), name=\"node_features_input\")\n",
    "    # 2) 多份邻接输入 [adj_r0, adj_r1, ...]，每个都可能是稀疏或稠密\n",
    "    adj_inputs = []\n",
    "    for r in range(num_relations):\n",
    "        adj_inputs.append(Input(shape=(None,), sparse=True, name=f\"adj_input_r{r}\"))\n",
    "\n",
    "    # 3) 需要预测的 (head, tail) 索引\n",
    "    head_indices = Input(shape=(), dtype=tf.int32, name=\"head_indices\")\n",
    "    tail_indices = Input(shape=(), dtype=tf.int32, name=\"tail_indices\")\n",
    "\n",
    "    # --------- GNN 计算 ---------\n",
    "    # 初始投影\n",
    "    x0 = Dense(hidden_dim, activation='relu')(node_features_input)   # (N, hidden_dim)\n",
    "\n",
    "    # 多关系卷积 (第一层)\n",
    "    x1 = MyRelationalConv(hidden_dim, num_relations)([x0] + adj_inputs)\n",
    "    x1 = Dropout(0.3)(x1)\n",
    "    x1 = tf.nn.relu(x1)\n",
    "\n",
    "    # 多关系卷积 (第二层)\n",
    "    x2 = MyRelationalConv(hidden_dim, num_relations)([x1] + adj_inputs)\n",
    "    x2 = Dropout(0.3)(x2)\n",
    "    x2 = tf.nn.relu(x2)\n",
    "\n",
    "    # 最终实体表示\n",
    "    node_embeddings = Dense(hidden_dim, activation='relu')(x2)\n",
    "\n",
    "    # --------- 解码 (link prediction) ---------\n",
    "    # 不再使用 KGE；只用 (head, tail) 点积做打分\n",
    "    h_emb = tf.gather(node_embeddings, head_indices)  # (batch_size, hidden_dim)\n",
    "    t_emb = tf.gather(node_embeddings, tail_indices)  # same shape\n",
    "    dot_score = tf.reduce_sum(h_emb * t_emb, axis=-1)  # (batch_size, )\n",
    "    output = tf.sigmoid(dot_score)\n",
    "\n",
    "    # --------- 封装成 Keras Model ---------\n",
    "    prediction_model = Model(\n",
    "        inputs=[node_features_input] + adj_inputs + [head_indices, tail_indices],\n",
    "        outputs=output\n",
    "    )\n",
    "\n",
    "    # 若需要单独得到节点嵌入，也可做个 embedding_model\n",
    "    embedding_model = Model(\n",
    "        inputs=[node_features_input] + adj_inputs,\n",
    "        outputs=node_embeddings\n",
    "    )\n",
    "    return prediction_model, embedding_model\n",
    "\n",
    "\n",
    "# ======================\n",
    "# 以下从“创建模型”往后的流程都与之前类似，\n",
    "# 只是把 create_link_prediction_model 替换为我们新的 create_multi_rel_gnn_no_kge\n",
    "# ======================\n",
    "\n",
    "print(\"\\n创建（无 KGE）多关系 GNN 模型...\")\n",
    "\n",
    "# 这里假设：\n",
    "#   1) num_relations > 1\n",
    "#   2) 你准备的多关系邻接矩阵放在 `adj_tensors` (list长度=num_relations)\n",
    "#   如果你只有一个 adj_tensor，就只能以 num_relations=1 方式调用\n",
    "#   或者你需要事先构建多份邻接矩阵\n",
    "num_entities = len(sampled_indices)       # 已采样后的实体数量\n",
    "node_features_dim = node_features.shape[1]\n",
    "hidden_dim = 32\n",
    "\n",
    "# 创建模型\n",
    "model, embedding_model = create_multi_rel_gnn_no_kge(\n",
    "    num_entities=num_entities,\n",
    "    num_relations=num_relations,      # 如果只有单一邻接，则可传 1\n",
    "    # num_relations=1,\n",
    "    node_features_dim=node_features_dim,\n",
    "    hidden_dim=hidden_dim\n",
    ")\n",
    "\n",
    "# 使用 ExponentialDecay 设置学习率调度\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.001,\n",
    "    decay_steps=1000,\n",
    "    decay_rate=0.95,\n",
    "    staircase=True\n",
    ")\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "# 编译模型\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "embedding_model.summary()\n",
    "\n",
    "# ==============\n",
    "# 下面的训练、验证、早停逻辑与之前一致\n",
    "# 只是注意：模型的输入签名变了\n",
    "#   [node_features, adj_r0, adj_r1, ..., head_indices, tail_indices]\n",
    "# 不再传递 relation_indices\n",
    "# ==============\n",
    "\n",
    "print(\"\\n尝试一次前向传播来检查形状...\")\n",
    "try:\n",
    "    # 生成一个小批量样本 (head, tail)，不需要 relation\n",
    "    sample_heads = np.array([0, 1], dtype=np.int32)\n",
    "    sample_tails = np.array([1, 0], dtype=np.int32)\n",
    "    # 如果你只有一份邻接，也可以写 [adj_tensor]\n",
    "    # 如果有多份，就像: [adj_tensors[0], adj_tensors[1], ...]\n",
    "    # 这里举例单一邻接:\n",
    "    predictions = model(\n",
    "        [node_features, adj_tensor, sample_heads, sample_tails],\n",
    "        training=False\n",
    "    )\n",
    "    print(f\"前向传播成功! 预测形状: {predictions.shape}\")\n",
    "\n",
    "    # 嵌入模型\n",
    "    embeddings = embedding_model(\n",
    "        [node_features, adj_tensor],\n",
    "        training=False\n",
    "    )\n",
    "    print(f\"嵌入模型前向传播成功! 节点嵌入形状: {embeddings.shape}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"前向传播出错: {e}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "准备训练和验证数据...\n",
      "训练集 样本数: 2384514；正样本: 1192257；负样本: 1192257\n",
      "验证集 样本数: 264946；正样本: 132473；负样本: 132473\n"
     ]
    }
   ],
   "source": [
    "#####################################\n",
    "# 7. 生成训练/验证数据 (和你之前一样)\n",
    "#    唯一不同：不再使用 relation 索引\n",
    "#####################################\n",
    "\n",
    "print(\"\\n准备训练和验证数据...\")\n",
    "\n",
    "# 这里假设 edges_src, edges_dst 是你训练集中的 (head, tail)，\n",
    "# num_entities = 采样后的实体数\n",
    "# generate_triplets 可以简化为只随机换头或尾，不再考虑关系。\n",
    "# 也可以保留，但把 relation 参数置空/去掉。\n",
    "def generate_triplets(heads, tails, num_entities, negative_ratio=1):\n",
    "    \"\"\"\n",
    "    输入正样本 (heads, tails)，\n",
    "    为每个正样本生成 negative_ratio 个负样本(随机替换头或尾)。\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    samples = []\n",
    "    \n",
    "    for i in range(len(heads)):\n",
    "        # 正样本\n",
    "        samples.append([heads[i], tails[i]])\n",
    "        labels.append(1)\n",
    "        \n",
    "        # 负样本\n",
    "        for _ in range(negative_ratio):\n",
    "            if np.random.random() < 0.5:\n",
    "                # 换头\n",
    "                fake_head = np.random.randint(0, num_entities)\n",
    "                while fake_head == heads[i]:\n",
    "                    fake_head = np.random.randint(0, num_entities)\n",
    "                samples.append([fake_head, tails[i]])\n",
    "            else:\n",
    "                # 换尾\n",
    "                fake_tail = np.random.randint(0, num_entities)\n",
    "                while fake_tail == tails[i]:\n",
    "                    fake_tail = np.random.randint(0, num_entities)\n",
    "                samples.append([heads[i], fake_tail])\n",
    "            labels.append(0)\n",
    "\n",
    "    samples = np.array(samples)\n",
    "    labels = np.array(labels)\n",
    "    return samples[:, 0], samples[:, 1], labels\n",
    "\n",
    "# 生成训练、验证集\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_idx, valid_idx = train_test_split(\n",
    "    range(len(edges_src)), test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "train_edges_src = edges_src[train_idx]\n",
    "train_edges_dst = edges_dst[train_idx]\n",
    "\n",
    "valid_edges_src = edges_src[valid_idx]\n",
    "valid_edges_dst = edges_dst[valid_idx]\n",
    "\n",
    "train_heads_all, train_tails_all, train_labels = generate_triplets(\n",
    "    train_edges_src, train_edges_dst, num_entities, negative_ratio=1\n",
    ")\n",
    "valid_heads_all, valid_tails_all, valid_labels = generate_triplets(\n",
    "    valid_edges_src, valid_edges_dst, num_entities, negative_ratio=1\n",
    ")\n",
    "\n",
    "print(f\"训练集 样本数: {len(train_labels)}；正样本: {np.sum(train_labels)}；负样本: {len(train_labels) - np.sum(train_labels)}\")\n",
    "print(f\"验证集 样本数: {len(valid_labels)}；正样本: {np.sum(valid_labels)}；负样本: {len(valid_labels) - np.sum(valid_labels)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "开始训练...\n",
      "Epoch 1/100\n",
      "Batch 11287/18630 - loss: 0.7203 - acc: 0.5003"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_30120\\1686967506.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[1;31m# 更新训练指标\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[0mtrain_loss_metric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m         \u001b[0mtrain_acc_metric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         print(f\"\\rBatch {b+1}/{num_batches} - \"\n",
      "\u001b[1;32mc:\\Users\\pm\\anaconda3\\envs\\310\\lib\\site-packages\\keras\\utils\\metrics_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(metric_obj, *args, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m                     \u001b[1;34m\"scope. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m                 )\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph_context_for_symbolic_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m             \u001b[0mupdate_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupdate_state_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mupdate_op\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# update_op will be None in eager execution.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[0mmetric_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mupdate_op\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdate_op\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\pm\\anaconda3\\envs\\310\\lib\\site-packages\\keras\\metrics\\base_metric.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    139\u001b[0m                 \u001b[0mcontrol_status\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_status_ctx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m                 ag_update_state = tf.__internal__.autograph.tf_convert(\n\u001b[0;32m    141\u001b[0m                     \u001b[0mobj_update_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontrol_status\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m                 )\n\u001b[1;32m--> 143\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mag_update_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\pm\\anaconda3\\envs\\310\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    686\u001b[0m           optional_features=optional_features)\n\u001b[0;32m    687\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    689\u001b[0m           \u001b[1;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 690\u001b[1;33m       \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    691\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ag_error_metadata'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    692\u001b[0m           \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    693\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\pm\\anaconda3\\envs\\310\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    327\u001b[0m     \u001b[0moptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcaller_fn_scope\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallopts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mconversion\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_in_allowlist_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Allowlisted %s: from cache'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 331\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    332\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_status_ctx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDISABLED\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Allowlisted: %s: AutoGraph is disabled in context'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\pm\\anaconda3\\envs\\310\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[0;32m    454\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mismethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__self__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTfMethodTarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    455\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__self__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 458\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    459\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\pm\\anaconda3\\envs\\310\\lib\\site-packages\\keras\\metrics\\base_metric.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, values, sample_weight)\u001b[0m\n\u001b[0;32m    499\u001b[0m                         \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight_ndim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m                     )\n\u001b[0;32m    501\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    502\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 503\u001b[1;33m         \u001b[0mvalue_sum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    504\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvalue_sum\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m             \u001b[0mupdate_total_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign_add\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue_sum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\pm\\anaconda3\\envs\\310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\pm\\anaconda3\\envs\\310\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1174\u001b[0m       \u001b[1;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1175\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1176\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1177\u001b[1;33m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1178\u001b[0m         \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m         \u001b[1;31m# TypeError, when given unexpected types.  So we need to catch both.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_dispatch_handler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\pm\\anaconda3\\envs\\310\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(input_tensor, axis, keepdims, name)\u001b[0m\n\u001b[0;32m   2309\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mend_compatibility\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2310\u001b[0m   \"\"\"\n\u001b[0;32m   2311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2312\u001b[0m   return reduce_sum_with_dims(input_tensor, axis, keepdims, name,\n\u001b[1;32m-> 2313\u001b[1;33m                               _ReductionDims(input_tensor, axis))\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\pm\\anaconda3\\envs\\310\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x, axis)\u001b[0m\n\u001b[0;32m   2149\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mx_rank\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2150\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_rank\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2151\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2152\u001b[0m       \u001b[1;31m# Otherwise, we rely on Range and Rank to do the right thing at run-time.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2153\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\pm\\anaconda3\\envs\\310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\pm\\anaconda3\\envs\\310\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1174\u001b[0m       \u001b[1;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1175\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1176\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1177\u001b[1;33m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1178\u001b[0m         \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m         \u001b[1;31m# TypeError, when given unexpected types.  So we need to catch both.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_dispatch_handler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\pm\\anaconda3\\envs\\310\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(start, limit, delta, dtype, name)\u001b[0m\n\u001b[0;32m   2120\u001b[0m     \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minferred_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2121\u001b[0m     \u001b[0mlimit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minferred_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2122\u001b[0m     \u001b[0mdelta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minferred_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2124\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_range\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\pm\\anaconda3\\envs\\310\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(start, limit, delta, name)\u001b[0m\n\u001b[0;32m   7733\u001b[0m         _ctx, \"Range\", name, start, limit, delta)\n\u001b[0;32m   7734\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7735\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7736\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7737\u001b[1;33m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7738\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7739\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7740\u001b[0m       return _range_eager_fallback(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "####################################\n",
    "# 8. 训练循环 (与之前大同小异)\n",
    "####################################\n",
    "\n",
    "train_loss_metric = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_acc_metric = tf.keras.metrics.BinaryAccuracy(name='train_acc')\n",
    "\n",
    "history = {\n",
    "    'loss': [],\n",
    "    'accuracy': [],\n",
    "    'val_loss': [],\n",
    "    'val_accuracy': []\n",
    "}\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_epoch = 0\n",
    "best_weights = None\n",
    "no_improvement_count = 0\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 100\n",
    "patience = 5\n",
    "min_delta = 0.001\n",
    "\n",
    "print(\"\\n开始训练...\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    \n",
    "    # 每轮重置\n",
    "    train_loss_metric.reset_states()\n",
    "    train_acc_metric.reset_states()\n",
    "    \n",
    "    # 打乱训练数据\n",
    "    idx_shuffle = np.random.permutation(len(train_labels))\n",
    "    train_heads_all_shuffled = train_heads_all[idx_shuffle]\n",
    "    train_tails_all_shuffled = train_tails_all[idx_shuffle]\n",
    "    train_labels_shuffled    = train_labels[idx_shuffle]\n",
    "    \n",
    "    num_batches = (len(train_labels_shuffled) + batch_size - 1) // batch_size\n",
    "\n",
    "    for b in range(num_batches):\n",
    "        start_idx = b * batch_size\n",
    "        end_idx = min((b+1) * batch_size, len(train_labels_shuffled))\n",
    "\n",
    "        batch_heads = train_heads_all_shuffled[start_idx:end_idx]\n",
    "        batch_tails = train_tails_all_shuffled[start_idx:end_idx]\n",
    "        batch_y     = train_labels_shuffled[start_idx:end_idx]\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # 前向传播\n",
    "            # 如果你有多份邻接，要传 [node_features] + [adj_tensors[i], ...] + [batch_heads, batch_tails]\n",
    "            # 这里假设只有一个 adj_tensor:\n",
    "            predictions = model([\n",
    "                node_features,    # (N, F)\n",
    "                adj_tensor,       # (N, N)\n",
    "                batch_heads,      # (batch_size,)\n",
    "                batch_tails\n",
    "            ], training=True)\n",
    "\n",
    "            loss = tf.keras.losses.binary_crossentropy(batch_y, predictions)\n",
    "            loss = tf.reduce_mean(loss)\n",
    "        \n",
    "        # 更新梯度\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        \n",
    "        # 更新训练指标\n",
    "        train_loss_metric.update_state(loss)\n",
    "        train_acc_metric.update_state(batch_y, predictions)\n",
    "\n",
    "        print(f\"\\rBatch {b+1}/{num_batches} - \"\n",
    "              f\"loss: {train_loss_metric.result():.4f} - \"\n",
    "              f\"acc: {train_acc_metric.result():.4f}\", end=\"\")\n",
    "\n",
    "    # 验证集评估\n",
    "    valid_loss_total = 0\n",
    "    valid_acc_total = 0\n",
    "    valid_len = len(valid_labels)\n",
    "    \n",
    "    valid_batch_size = 512\n",
    "    valid_num_batches = (valid_len + valid_batch_size - 1) // valid_batch_size\n",
    "    \n",
    "    for vb in range(valid_num_batches):\n",
    "        vstart = vb * valid_batch_size\n",
    "        vend   = min((vb+1)*valid_batch_size, valid_len)\n",
    "        \n",
    "        batch_heads = valid_heads_all[vstart:vend]\n",
    "        batch_tails = valid_tails_all[vstart:vend]\n",
    "        batch_y     = valid_labels[vstart:vend]\n",
    "\n",
    "        val_preds = model([\n",
    "            node_features,\n",
    "            adj_tensor,\n",
    "            batch_heads,\n",
    "            batch_tails\n",
    "        ], training=False)\n",
    "\n",
    "        b_loss = tf.reduce_mean(tf.keras.losses.binary_crossentropy(batch_y, val_preds))\n",
    "        b_acc  = tf.reduce_mean(\n",
    "            tf.cast(tf.equal(tf.round(val_preds), batch_y), tf.float32)\n",
    "        )\n",
    "\n",
    "        batch_size_current = vend - vstart\n",
    "        valid_loss_total += b_loss.numpy() * batch_size_current\n",
    "        valid_acc_total  += b_acc.numpy()  * batch_size_current\n",
    "    \n",
    "    valid_loss = valid_loss_total / valid_len\n",
    "    valid_acc  = valid_acc_total  / valid_len\n",
    "    \n",
    "    train_loss_value = train_loss_metric.result().numpy()\n",
    "    train_acc_value  = train_acc_metric.result().numpy()\n",
    "\n",
    "    history['loss'].append(train_loss_value)\n",
    "    history['accuracy'].append(train_acc_value)\n",
    "    history['val_loss'].append(valid_loss)\n",
    "    history['val_accuracy'].append(valid_acc)\n",
    "\n",
    "    print(f\"\\rEpoch {epoch+1}/{epochs} - \"\n",
    "          f\"loss: {train_loss_value:.4f} - acc: {train_acc_value:.4f} - \"\n",
    "          f\"val_loss: {valid_loss:.4f} - val_acc: {valid_acc:.4f}\")\n",
    "\n",
    "    # 早停\n",
    "    if valid_loss < best_val_loss - min_delta:\n",
    "        best_val_loss = valid_loss\n",
    "        best_epoch = epoch\n",
    "        best_weights = model.get_weights()\n",
    "        no_improvement_count = 0\n",
    "        print(f\"  ✓ 新的最佳模型! (val_loss: {best_val_loss:.4f})\")\n",
    "    else:\n",
    "        no_improvement_count += 1\n",
    "        print(f\"  ✗ 验证损失未改善 ({no_improvement_count}/{patience})\")\n",
    "    \n",
    "    if no_improvement_count >= patience:\n",
    "        print(f\"\\n早停! 在 {patience} 个 epoch 内验证损失未改善。\")\n",
    "        print(f\"回到 epoch={best_epoch+1} 权重 (val_loss={best_val_loss:.4f})\")\n",
    "        model.set_weights(best_weights)\n",
    "        break\n",
    "\n",
    "# 恢复最佳权重\n",
    "if best_weights is not None:\n",
    "    model.set_weights(best_weights)\n",
    "    print(f\"使用最佳权重 (来自 epoch {best_epoch+1}, val_loss={best_val_loss:.4f})\")\n",
    "\n",
    "\n",
    "#####################################\n",
    "# 9. 保存模型 & 可视化\n",
    "#####################################\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "MODEL_DIR = \"../saved_model\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "model_path = os.path.join(MODEL_DIR, \"best_kg_model_no_kge\")\n",
    "model.save_weights(model_path)\n",
    "print(f\"模型权重已保存: {model_path}\")\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['loss'], label='训练损失')\n",
    "plt.plot(history['val_loss'], label='验证损失')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history['accuracy'], label='训练准确率')\n",
    "plt.plot(history['val_accuracy'], label='验证准确率')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(MODEL_DIR, 'training_history_no_kge.png'))\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n训练完成！（无 KGE 版本）\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
